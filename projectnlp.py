# -*- coding: utf-8 -*-
"""ProjectNLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZzDUPX03ZUBqx_a7BMYv1gWaQUj-BwjB
"""

import numpy as np
import pandas as pd
import nltk
import random
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# Download NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import stopwords, wordnet

# Load datasets
gossipcop_fake = pd.read_csv('gossipcop_fake.csv')
gossipcop_real = pd.read_csv('gossipcop_real.csv')
politifact_fake = pd.read_csv('politifact_fake.csv')
politifact_real = pd.read_csv('politifact_real.csv')

# Combine datasets
gossipcop_fake['label'] = 0
gossipcop_real['label'] = 1
politifact_fake['label'] = 0
politifact_real['label'] = 1

data = pd.concat([gossipcop_fake, gossipcop_real, politifact_fake, politifact_real], ignore_index=True)

# Data preprocessing
stop_words = set(stopwords.words('english'))

def clean_text(text):
    words = text.lower().split()
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

data['cleaned_title'] = data['title'].apply(clean_text)

# Text Data Augmentation Functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonym = lemma.name().replace('_', ' ')
            if synonym != word:
                synonyms.add(synonym)
    return list(synonyms)

def synonym_replacement(sentence, n):
    words = sentence.split()
    if len(words) == 0:
        return sentence
    new_words = words.copy()
    random_word_list = list(set(words))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(synonyms)
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break
    sentence = ' '.join(new_words)
    return sentence

def random_insertion(sentence, n):
    words = sentence.split()
    if len(words) == 0:
        return sentence
    for _ in range(n):
        add_word(words)
    return ' '.join(words)

def add_word(words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = words[random.randint(0, len(words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = synonyms[0]
    random_idx = random.randint(0, len(words)-1)
    words.insert(random_idx, random_synonym)

def random_swap(sentence, n):
    words = sentence.split()
    if len(words) == 0:
        return sentence
    for _ in range(n):
        words = swap_word(words)
    return ' '.join(words)

def swap_word(words):
    random_idx_1 = random.randint(0, len(words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(words)-1)
        counter += 1
        if counter > 3:
            return words
    words[random_idx_1], words[random_idx_2] = words[random_idx_2], words[random_idx_1]
    return words

def random_deletion(sentence, p):
    words = sentence.split()
    if len(words) <= 1:  # If there's only one word or none, return the original sentence
        return ' '.join(words)

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        return ' '.join([random.choice(words)])  # Return a random word from original words if all deleted

    return ' '.join(new_words)

def augment_sentence(sentence):
    augmented_sentences = []
    augmented_sentences.append(synonym_replacement(sentence, n=2))
    augmented_sentences.append(random_insertion(sentence, n=2))
    augmented_sentences.append(random_swap(sentence, n=2))
    augmented_sentences.append(random_deletion(sentence, p=0.2))
    return augmented_sentences

# Apply data augmentation
augmented_data = []
for _, row in data.iterrows():
    augmented_sentences = augment_sentence(row['cleaned_title'])
    for aug_sentence in augmented_sentences:
        augmented_data.append({'title': aug_sentence, 'label': row['label']})

augmented_df = pd.DataFrame(augmented_data)

# Combine original and augmented data
combined_data = pd.concat([data[['cleaned_title', 'label']].rename(columns={'cleaned_title': 'title'}), augmented_df], ignore_index=True)

# Tokenization and padding
max_words = 5000
max_len = 100

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(combined_data['title'])
sequences = tokenizer.texts_to_sequences(combined_data['title'])
padded_sequences = pad_sequences(sequences, maxlen=max_len)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, combined_data['label'], test_size=0.2, random_state=42)

# Model definition
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))
model.add(Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(32, kernel_regularizer=l2(0.01))))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

"""


##batch_size=512"""

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train model with validation split
history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_split=0.2, callbacks=[early_stopping])

# Save the trained model
model.save('fake_news_detection_model_512.h5')
# Save only the weights
model.save_weights('fake_news_detection_weights_512.h5')

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Model Accuracy: {accuracy}')

# Plot training and validation accuracy and loss
def plot_history(history):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()

plot_history(history)

# Classification report
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))

# Define a function to preprocess and predict
def predict_fake_news(statement, model, tokenizer, max_len):
    def clean_text(text):
        stop_words = set(stopwords.words('english'))
        words = text.lower().split()
        words = [word for word in words if word.isalpha() and word not in stop_words]
        return ' '.join(words)

    # Preprocess the statement
    cleaned_statement = clean_text(statement)
    sequence = tokenizer.texts_to_sequences([cleaned_statement])
    padded_sequence = pad_sequences(sequence, maxlen=max_len)

    # Predict
    prediction = model.predict(padded_sequence)[0][0]
    return 'Real' if prediction >= 0.5 else 'Fake'

# Function to predict and display results for multiple statements
def test_multiple_statements(statements, model, tokenizer, max_len):
    for statement in statements:
        result = predict_fake_news(statement, model, tokenizer, max_len)
        print(f'The statement: "{statement}" is classified as: {result}')


# List of statements to test
test_statements = [
    "NASA confirms liquid water on Mars",
    "Celebrity endorses miracle weight loss pill",
    "New technology promises to solve world hunger",
    "Aliens have landed on Earth, says government",
    "Economic growth predicted to soar in 2025",
    "Scientists discover new species of dinosaur",
    "Study shows chocolate cures cancer",
    "Politician caught in corruption scandal",
    "Local hero saves family from burning building",
    "New movie breaks box office records",
    "Scientists prove that drinking bleach can cure COVID-19.",
"Millions of illegal votes were cast in the last election, says anonymous source.",
"New study claims that climate change is a hoax perpetuated by scientists for profit.",
"Elvis Presley spotted working as a cashier in a small-town grocery store.",
"Alien spacecraft discovered on the dark side of the moon, NASA confirms.",
"World leaders gather secretly to plan global domination, leaked documents reveal.",
"New app guarantees to make you a millionaire overnight with just a few clicks.",
"Facebook to start charging users $2.99/month for access to their accounts.",
"Vaccines proven to cause autism, says disgraced former doctor.",
"Breaking: Loch Ness Monster captured alive, shocking footage released."
]

# Test the model with multiple statements
test_multiple_statements(test_statements, model, tokenizer, max_len)

"""##batch_size=128"""

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train model with validation split
history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_split=0.2, callbacks=[early_stopping])

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Model Accuracy: {accuracy}')

# Plot training and validation accuracy and loss
def plot_history(history):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()

plot_history(history)

# Classification report
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))

# Save the trained model
model.save('fake_news_detection_model.h5')

# Define a function to preprocess and predict
def predict_fake_news(statement, model, tokenizer, max_len):
    def clean_text(text):
        stop_words = set(stopwords.words('english'))
        words = text.lower().split()
        words = [word for word in words if word.isalpha() and word not in stop_words]
        return ' '.join(words)

    # Preprocess the statement
    cleaned_statement = clean_text(statement)
    sequence = tokenizer.texts_to_sequences([cleaned_statement])
    padded_sequence = pad_sequences(sequence, maxlen=max_len)

    # Predict
    prediction = model.predict(padded_sequence)[0][0]
    return 'Real' if prediction >= 0.5 else 'Fake'

# Function to predict and display results for multiple statements
def test_multiple_statements(statements, model, tokenizer, max_len):
    for statement in statements:
        result = predict_fake_news(statement, model, tokenizer, max_len)
        print(f'The statement: "{statement}" is classified as: {result}')


# List of statements to test
test_statements = [
    "NASA confirms liquid water on Mars",
    "Celebrity endorses miracle weight loss pill",
    "New technology promises to solve world hunger",
    "Aliens have landed on Earth, says government",
    "Economic growth predicted to soar in 2025",
    "Scientists discover new species of dinosaur",
    "Study shows chocolate cures cancer",
    "Politician caught in corruption scandal",
    "Local hero saves family from burning building",
    "New movie breaks box office records",
    "Scientists prove that drinking bleach can cure COVID-19.",
"Millions of illegal votes were cast in the last election, says anonymous source.",
"New study claims that climate change is a hoax perpetuated by scientists for profit.",
"Elvis Presley spotted working as a cashier in a small-town grocery store.",
"Alien spacecraft discovered on the dark side of the moon, NASA confirms.",
"World leaders gather secretly to plan global domination, leaked documents reveal.",
"New app guarantees to make you a millionaire overnight with just a few clicks.",
"Facebook to start charging users $2.99/month for access to their accounts.",
"Vaccines proven to cause autism, says disgraced former doctor.",
"Breaking: Loch Ness Monster captured alive, shocking footage released."
]

# Test the model with multiple statements
test_multiple_statements(test_statements, model, tokenizer, max_len)



"""##batch_size=64"""

import numpy as np
import pandas as pd
import nltk
import random
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# Download NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

from nltk.corpus import stopwords, wordnet

# Load datasets
gossipcop_fake = pd.read_csv('gossipcop_fake.csv')
gossipcop_real = pd.read_csv('gossipcop_real.csv')
politifact_fake = pd.read_csv('politifact_fake.csv')
politifact_real = pd.read_csv('politifact_real.csv')

# Combine datasets
gossipcop_fake['label'] = 0
gossipcop_real['label'] = 1
politifact_fake['label'] = 0
politifact_real['label'] = 1

data = pd.concat([gossipcop_fake, gossipcop_real, politifact_fake, politifact_real], ignore_index=True)

# Data preprocessing
stop_words = set(stopwords.words('english'))

def clean_text(text):
    words = text.lower().split()
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

data['cleaned_title'] = data['title'].apply(clean_text)

# Text Data Augmentation Functions
def get_synonyms(word):
    synonyms = set()
    for syn in wordnet.synsets(word):
        for lemma in syn.lemmas():
            synonym = lemma.name().replace('_', ' ')
            if synonym != word:
                synonyms.add(synonym)
    return list(synonyms)

def synonym_replacement(sentence, n):
    words = sentence.split()
    if len(words) == 0:
        return sentence
    new_words = words.copy()
    random_word_list = list(set(words))
    random.shuffle(random_word_list)
    num_replaced = 0
    for random_word in random_word_list:
        synonyms = get_synonyms(random_word)
        if len(synonyms) >= 1:
            synonym = random.choice(synonyms)
            new_words = [synonym if word == random_word else word for word in new_words]
            num_replaced += 1
        if num_replaced >= n:
            break
    sentence = ' '.join(new_words)
    return sentence

def random_insertion(sentence, n):
    words = sentence.split()
    if len(words) == 0:
        return sentence
    for _ in range(n):
        add_word(words)
    return ' '.join(words)

def add_word(words):
    synonyms = []
    counter = 0
    while len(synonyms) < 1:
        random_word = words[random.randint(0, len(words)-1)]
        synonyms = get_synonyms(random_word)
        counter += 1
        if counter >= 10:
            return
    random_synonym = synonyms[0]
    random_idx = random.randint(0, len(words)-1)
    words.insert(random_idx, random_synonym)

def random_swap(sentence, n):
    words = sentence.split()
    if len(words) == 0:
        return sentence
    for _ in range(n):
        words = swap_word(words)
    return ' '.join(words)

def swap_word(words):
    random_idx_1 = random.randint(0, len(words)-1)
    random_idx_2 = random_idx_1
    counter = 0
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(words)-1)
        counter += 1
        if counter > 3:
            return words
    words[random_idx_1], words[random_idx_2] = words[random_idx_2], words[random_idx_1]
    return words

def random_deletion(sentence, p):
    words = sentence.split()
    if len(words) <= 1:  # If there's only one word or none, return the original sentence
        return ' '.join(words)

    new_words = []
    for word in words:
        r = random.uniform(0, 1)
        if r > p:
            new_words.append(word)

    if len(new_words) == 0:
        return ' '.join([random.choice(words)])  # Return a random word from original words if all deleted

    return ' '.join(new_words)

def augment_sentence(sentence):
    augmented_sentences = []
    augmented_sentences.append(synonym_replacement(sentence, n=2))
    augmented_sentences.append(random_insertion(sentence, n=2))
    augmented_sentences.append(random_swap(sentence, n=2))
    augmented_sentences.append(random_deletion(sentence, p=0.2))
    return augmented_sentences

# Apply data augmentation
augmented_data = []
for _, row in data.iterrows():
    augmented_sentences = augment_sentence(row['cleaned_title'])
    for aug_sentence in augmented_sentences:
        augmented_data.append({'title': aug_sentence, 'label': row['label']})

augmented_df = pd.DataFrame(augmented_data)

# Combine original and augmented data
combined_data = pd.concat([data[['cleaned_title', 'label']].rename(columns={'cleaned_title': 'title'}), augmented_df], ignore_index=True)

# Tokenization and padding
max_words = 5000
max_len = 100

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(combined_data['title'])
sequences = tokenizer.texts_to_sequences(combined_data['title'])
padded_sequences = pad_sequences(sequences, maxlen=max_len)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, combined_data['label'], test_size=0.2, random_state=42)

# Model definition
model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))
model.add(Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.01))))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(32, kernel_regularizer=l2(0.01))))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train model with validation split
history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])

# Evaluate model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Model Accuracy: {accuracy}')

# Plot training and validation accuracy and loss
def plot_history(history):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()

plot_history(history)

# Classification report
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))

# Save the trained model
model.save('fake_news_detection_model.h5')

# Define a function to preprocess and predict
def predict_fake_news(statement, model, tokenizer, max_len):
    def clean_text(text):
        stop_words = set(stopwords.words('english'))
        words = text.lower().split()
        words = [word for word in words if word.isalpha() and word not in stop_words]
        return ' '.join(words)

    # Preprocess the statement
    cleaned_statement = clean_text(statement)
    sequence = tokenizer.texts_to_sequences([cleaned_statement])
    padded_sequence = pad_sequences(sequence, maxlen=max_len)

    # Predict
    prediction = model.predict(padded_sequence)[0][0]
    return 'Real' if prediction >= 0.5 else 'Fake'

# Function to predict and display results for multiple statements
def test_multiple_statements(statements, model, tokenizer, max_len):
    for statement in statements:
        result = predict_fake_news(statement, model, tokenizer, max_len)
        print(f'The statement: "{statement}" is classified as: {result}')


# List of statements to test
test_statements = [
    "NASA confirms liquid water on Mars",
    "Celebrity endorses miracle weight loss pill",
    "New technology promises to solve world hunger",
    "Aliens have landed on Earth, says government",
    "Economic growth predicted to soar in 2025",
    "Scientists discover new species of dinosaur",
    "Study shows chocolate cures cancer",
    "Politician caught in corruption scandal",
    "Local hero saves family from burning building",
    "New movie breaks box office records",
    "Scientists prove that drinking bleach can cure COVID-19.",
"Millions of illegal votes were cast in the last election, says anonymous source.",
"New study claims that climate change is a hoax perpetuated by scientists for profit.",
"Elvis Presley spotted working as a cashier in a small-town grocery store.",
"Alien spacecraft discovered on the dark side of the moon, NASA confirms.",
"World leaders gather secretly to plan global domination, leaked documents reveal.",
"New app guarantees to make you a millionaire overnight with just a few clicks.",
"Facebook to start charging users $2.99/month for access to their accounts.",
"Vaccines proven to cause autism, says disgraced former doctor.",
"Breaking: Loch Ness Monster captured alive, shocking footage released."
]

# Test the model with multiple statements
test_multiple_statements(test_statements, model, tokenizer, max_len)